{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Suh_TweetEstimator.ipynb","provenance":[{"file_id":"1SSiPoiweWjmXOv5EYCmeH0bhX-iGbo0A","timestamp":1611794154895},{"file_id":"1EaBDbrjzFDKPccmF8Cj464gCMHdpKtNq","timestamp":1611195248624}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kG6ba8jPpZay","colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"status":"error","timestamp":1617938781224,"user_tz":420,"elapsed":338911,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"e64bc8d3-1a8f-4e16-acdf-4c34e8f8de31"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n","\n","Enter your authorization code:\n","4/1AY0e-g7FLXJia1tBYGdLJSCOahs3An9qHQC8Y16lt7QzdSKJP8KwJtPWD30\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mdrive_exited\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0moauth_failed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mdomain_disabled_drivefs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     ])\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect\u001b[0;34m(self, pattern, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcompiled_pattern_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_pattern_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         return self.expect_list(compiled_pattern_list,\n\u001b[0;32m--> 344\u001b[0;31m                 timeout, searchwindowsize, async_)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     def expect_list(self, pattern_list, timeout=-1, searchwindowsize=-1,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpect_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     def expect_exact(self, pattern_list, timeout=-1, searchwindowsize=-1,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Keep reading until exception or return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"As1t2-pspusG"},"source":["navigate to Case Research folder"]},{"cell_type":"code","metadata":{"id":"Njt9o_n_qiJh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611794252062,"user_tz":480,"elapsed":355,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"93dbffca-eaf6-48aa-dfc8-87feea0982ac"},"source":["%cd /content/drive/MyDrive/twitterScrape"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/twitterScrape\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s4c-uc_FtoCk"},"source":["Set Tensorflow to  1.15"]},{"cell_type":"code","metadata":{"id":"byDXXVHosdq0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611794253867,"user_tz":480,"elapsed":514,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"c3d2ca03-1e55-4181-ab32-74f2db08c554"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_pSyGXyWtlGI"},"source":["Install needed libraries here"]},{"cell_type":"code","metadata":{"id":"2tVzmr_ttfPk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611794273598,"user_tz":480,"elapsed":18924,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"6b703914-4c6b-4a92-a69f-4485018e04fb"},"source":["!apt install -qq enchant\n","!pip install pyenchant"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following additional packages will be installed:\n","  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n","  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n","Suggested packages:\n","  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n","  | openoffice.org-core libenchant-voikko\n","The following NEW packages will be installed:\n","  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n","  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n","0 upgraded, 10 newly installed, 0 to remove and 13 not upgraded.\n","Need to get 1,310 kB of archives.\n","After this operation, 5,353 kB of additional disk space will be used.\n","Preconfiguring packages ...\n","Selecting previously unselected package libtext-iconv-perl.\n","(Reading database ... 146374 files and directories currently installed.)\n","Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n","Unpacking libtext-iconv-perl (1.7-5build6) ...\n","Selecting previously unselected package libaspell15:amd64.\n","Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n","Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n","Selecting previously unselected package emacsen-common.\n","Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n","Unpacking emacsen-common (2.0.8) ...\n","Selecting previously unselected package dictionaries-common.\n","Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n","Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n","Unpacking dictionaries-common (1.27.2) ...\n","Selecting previously unselected package aspell.\n","Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n","Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n","Selecting previously unselected package aspell-en.\n","Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n","Unpacking aspell-en (2017.08.24-0-0.1) ...\n","Selecting previously unselected package hunspell-en-us.\n","Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n","Unpacking hunspell-en-us (1:2017.08.24) ...\n","Selecting previously unselected package libhunspell-1.6-0:amd64.\n","Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n","Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n","Selecting previously unselected package libenchant1c2a:amd64.\n","Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n","Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n","Selecting previously unselected package enchant.\n","Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n","Unpacking enchant (1.6.0-11.1) ...\n","Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n","Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n","Setting up emacsen-common (2.0.8) ...\n","Setting up libtext-iconv-perl (1.7-5build6) ...\n","Setting up dictionaries-common (1.27.2) ...\n","Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n","Setting up hunspell-en-us (1:2017.08.24) ...\n","Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n","Setting up aspell-en (2017.08.24-0-0.1) ...\n","Setting up enchant (1.6.0-11.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for dictionaries-common (1.27.2) ...\n","aspell-autobuildhash: processing: en [en-common].\n","aspell-autobuildhash: processing: en [en-variant_0].\n","aspell-autobuildhash: processing: en [en-variant_1].\n","aspell-autobuildhash: processing: en [en-variant_2].\n","aspell-autobuildhash: processing: en [en-w_accents-only].\n","aspell-autobuildhash: processing: en [en-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-variant_0].\n","aspell-autobuildhash: processing: en [en_AU-variant_1].\n","aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n","aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-variant_0].\n","aspell-autobuildhash: processing: en [en_CA-variant_1].\n","aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n","aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n","aspell-autobuildhash: processing: en [en_GB-variant_0].\n","aspell-autobuildhash: processing: en [en_GB-variant_1].\n","aspell-autobuildhash: processing: en [en_US-w_accents-only].\n","aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n","Collecting pyenchant\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8c/bd224a5db562ac008edbfaf015f5d5c98ea13e745247cd4ab5fc5b683085/pyenchant-3.2.0-py3-none-any.whl (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 4.7MB/s \n","\u001b[?25hInstalling collected packages: pyenchant\n","Successfully installed pyenchant-3.2.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QVwW1-uqryRs"},"source":["TwitterClassify"]},{"cell_type":"code","metadata":{"id":"zLn9r1GnrVnE","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1611794431153,"user_tz":480,"elapsed":8508,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"a6cd3d43-2d38-46a2-9aa0-2b123ae14bf4"},"source":["import string\n","import time\n","import os\n","import tensorflow as tf\n","from tensorflow.contrib import predictor\n","import numpy as np\n","import pandas as pd\n","import os\n","import enchant\n","import csv\n","\n","\n","def _bytes_feature(value):\n","    bValue = bytes(value, 'utf-8')\n","    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[bValue]))\n","\n","\n","def runPrediction(msg, predModel):\n","    nMsg = msg;\n","\n","    feature = {\n","        'tweetText': _bytes_feature(nMsg)\n","    }\n","    example = tf.train.Example(\n","        features=tf.train.Features(\n","            feature=feature\n","        )\n","    )\n","    serialized_example = example.SerializeToString()\n","    results = predModel({\"inputs\": [serialized_example]})\n","    results = results.get('scores')[0]\n","    print(results)\n","    results = str(results).strip('[')\n","    results = str(results).strip(']')\n","    return results\n","\n","def main():\n","    top_emotions = [\"Anger\",\"Anticipation\",\"Disgust\",\"Joy\",\"Fear\",\"Surprise\",\"Sadness\"]\n","    top_emotions.sort()\n","    headCats = [\"emotions\",\"tweetText\"]\n","    headCats.extend(top_emotions)\n","    en_us = enchant.Dict(\"en_US\")\n","    #this is the output file\n","    csvFile = open('twitter_userScore.csv', 'w', newline='')\n","    csvWriter = csv.writer(csvFile)\n","    csvWriter.writerow(headCats)\n","    #load trained model\n","    predict_fn = predictor.from_saved_model(\"export/1611196130\")\n","\n","    #this is the one to load\n","    data = pd.read_csv('twitter_emotions.csv', header=0)\n","    #np.random.shuffle(data.values[1:, :])\n","    data.head()\n","    # tweetText is the text being evaluated\n","    tweetText = data['tweetText']\n","    data['emotions'] = data['emotions'].str.strip('[')\n","    data['emotions'] = data['emotions'].str.strip(']')\n","    data['emotions'] = data['emotions'].str.replace(\" \", \"\")\n","    data['emotions'] = data['emotions'].str.split(',')\n","    user = data['emotions']\n","    for i in range(len(tweetText)):\n","\n","        if isinstance(tweetText[i], str):\n","            pMsg = runPrediction(tweetText[i], predict_fn)\n","            sMsg = pMsg.split(\" \")\n","            outMsg = [user[i],tweetText[i]]\n","            for m in sMsg:\n","                if len(m) > 0:\n","                    outMsg.append(m)\n","            csvWriter.writerow(outMsg)\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-eccc76d9bb5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-eccc76d9bb5b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mcsvWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadCats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#load trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mpredict_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"export/1611196130\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#this is the one to load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/contrib/predictor/predictor_factories.py\u001b[0m in \u001b[0;36mfrom_saved_model\u001b[0;34m(export_dir, signature_def_key, signature_def, input_names, output_names, tags, graph, config)\u001b[0m\n\u001b[1;32m    151\u001b[0m       \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       config=config)\n\u001b[0m","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/contrib/predictor/saved_model_predictor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, export_dir, signature_def_key, signature_def, input_names, output_names, tags, graph, config)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(sess, tags, export_dir, import_scope, **saver_kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMetaGraphDef\u001b[0m \u001b[0massociated\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtags\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m   \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m   \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSavedModelLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msaver_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, export_dir)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexport_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m                   (export_dir,\n\u001b[1;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: export/1611196130/{saved_model.pbtxt|saved_model.pb}"]}]},{"cell_type":"markdown","metadata":{"id":"jSp_pUYCuint"},"source":["Train the estimator"]},{"cell_type":"code","metadata":{"id":"toOAzQ2fsZP6","colab":{"base_uri":"https://localhost:8080/","height":596},"executionInfo":{"status":"error","timestamp":1611794123843,"user_tz":480,"elapsed":7203,"user":{"displayName":"Casey Rehm","photoUrl":"","userId":"14928713571001354102"}},"outputId":"d6fd6732-f725-402b-f492-44e481cc3ded"},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from ast import literal_eval\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","\n","\n","def main():\n","\n","\n","    data = pd.read_csv('twitter.csv', header = 0)\n","    np.random.shuffle(data.values[1:, :])\n","    data.head()\n","    descriptions = data['tweetText']\n","    data['emotions'] = data['emotions'].str.strip('[')\n","    data['emotions'] = data['emotions'].str.strip(']')\n","    data['emotions'] = data['emotions'].str.replace(\" \", \"\")\n","    data['emotions'] = data['emotions'].str.split(',')\n","\n","    emotion = data['emotions']\n","\n","    emotionTest = data['emotions']\n","    emotionTest = list(set(emotionTest))\n","    emotionTest.sort()\n","\n","\n","    top_emotions = [\"Anger\",\"Happy\",\"Sad\",\"Surprise\",\"Fear\",\"Disgust\",\"Excited\"]\n","    top_emotions.sort()\n","    print(len(top_emotions))\n","    print(top_emotions)\n","    print(emotionTest)\n","    train_size = int(len(descriptions)* .8)\n","    train_descriptions = descriptions[:train_size].astype('str')\n","    train_emotions = emotion[:train_size]\n","\n","    test_descriptions = descriptions[train_size:].astype('str')\n","    test_emotions = emotion[train_size:]\n","\n","    description_embeddings = hub.text_embedding_column(\n","      \"tweetText\",\n","      module_spec=\"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n","    )\n","\n","    encoder = MultiLabelBinarizer()\n","    encoder.fit_transform(train_emotions)\n","    train_encoded = encoder.transform(train_emotions)\n","    test_encoded = encoder.transform(test_emotions)\n","    num_classes = len(encoder.classes_)\n","    print(num_classes)\n","    print(encoder.classes_)\n","    print(train_encoded[0])\n","\n","    multi_label_head = tf.contrib.estimator.multi_label_head(\n","        num_classes,\n","        loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n","    )\n","\n","    feature_columns = [description_embeddings]\n","    estimator = tf.estimator.DNNEstimator(\n","        head=multi_label_head,\n","        hidden_units=[64,10],\n","        feature_columns=feature_columns,\n","        model_dir='models/twitterEmot'\n","    )\n","\n","    # Format our data for the numpy_input_fn\n","    features = {\n","      'tweetText': np.array(train_descriptions)\n","    }\n","    labels = np.array(train_encoded)\n","\n","    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n","        features,\n","        labels,\n","        shuffle=True,\n","        batch_size=32,\n","        num_epochs=200,\n","    )\n","\n","\n","    estimator.train(input_fn=train_input_fn)\n","    feature_spec = tf.feature_column.make_parse_example_spec(feature_columns)\n","    print(feature_spec)\n","    # Build receiver function, and export.\n","    serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n","    export_dir = estimator.export_savedmodel('export', serving_input_receiver_fn)\n","\n","    eval_input_fn = tf.estimator.inputs.numpy_input_fn({\"tweetText\": np.array(test_descriptions).astype(np.str)}, test_encoded.astype(np.int32), shuffle=False)\n","\n","    estimator.evaluate(input_fn=eval_input_fn)\n","    raw_test = [\n","        \"I am very angry at google colab\",\n","        # Documentary\n","        \"I really like ice cream\",\n","        # Action, Adventure\n","        \"I'm afraid of spiders\",\n","        # Comedy\n","    ]\n","    # Generate predictions\n","    predict_input_fn = tf.estimator.inputs.numpy_input_fn({\"tweetText\": np.array(raw_test).astype(np.str)}, shuffle=False)\n","    results = estimator.predict(predict_input_fn)\n","    mCnt = 0\n","    for tweetEmotions in results:\n","        print(raw_test[mCnt].encode(encoding='UTF-8'))\n","        top_2 = tweetEmotions['probabilities'].argsort()[-2:][::-1]\n","        for emot in top_2:\n","            text_emot = encoder.classes_[emot]\n","            print(text_emot + ': ' + str(round(tweetEmotions['probabilities'][emot] * 100, 2)) + '%')\n","        print('')\n","        mCnt = mCnt + 1\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'tweetText'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-21ccad82004f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-21ccad82004f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdescriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweetText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emotions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emotions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'['\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emotions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emotions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m']'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'tweetText'"]}]},{"cell_type":"code","metadata":{"id":"KvMEQlB2u9xT"},"source":[""],"execution_count":null,"outputs":[]}]}