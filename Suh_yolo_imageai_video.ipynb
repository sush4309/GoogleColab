{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Suh_yolo_imageai_video.ipynb","provenance":[{"file_id":"1SB7fs9P2rrfpDwJktGPC3FIpYhPpXRB-","timestamp":1607385912106},{"file_id":"https://github.com/aissam-out/YOLO/blob/master/YOLO_ImageAI_video.ipynb","timestamp":1607317645024}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Y4EXVKawm3rP"},"source":["# Video detection with ImageAI and YOLOv3"]},{"cell_type":"markdown","metadata":{"id":"8uGRdi-om6wH"},"source":["**ImageAI** provided very powerful yet easy to use classes and functions to perform Video Object Detection and Tracking and Video analysis. \n","\n","**ImageAI** allows you to perform all of these with state-of-the-art deep learning algorithms like RetinaNet, YOLOv3 and TinyYOLOv3. \n","\n","With ImageAI you can run detection tasks and analyse videos and live-video feeds from device cameras and IP cameras. \n","\n","In this tutorial we will implement a case study using YOLOv3 over a stored video. The code is inspired by the [ImageAI documentation](https://imageai.readthedocs.io/en/latest/video/index.html)"]},{"cell_type":"markdown","metadata":{"id":"tXvkvSC8eij8"},"source":["## Install & Load the resources"]},{"cell_type":"code","metadata":{"id":"OjOia9mPm-w2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622673235844,"user_tz":420,"elapsed":242,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"6b5dca87-ede1-42ea-8dce-0cfe5cb43179"},"source":["%tensorflow_version 1.x"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uIMtTTlXfqm","executionInfo":{"status":"ok","timestamp":1622673245964,"user_tz":420,"elapsed":8800,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"f5280268-47a2-4bd1-d810-94873d2e05b8"},"source":["!pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl\n","!pip install -q opencv-python\n","!pip install -q pillow"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting imageai==2.0.2\n","\u001b[?25l  Downloading https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl (151kB)\n","\u001b[K     |████████████████████████████████| 153kB 2.1MB/s \n","\u001b[?25hInstalling collected packages: imageai\n","Successfully installed imageai-2.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yw_KQVliZi75","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622673251237,"user_tz":420,"elapsed":3814,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"3dd92410-e40a-42b3-f98a-2b2a9a47f28d"},"source":["from imageai.Detection import VideoObjectDetection\n","from imageai.Detection import ObjectDetection\n","import matplotlib as plt\n","import tensorflow as tf\n","import numpy as np\n","import scipy\n","import keras\n","import h5py"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"DRHBJBtXe1Yp"},"source":["## Create an instance of the VideoObjectDetection"]},{"cell_type":"code","metadata":{"id":"Xz5eb3mAZvpz","executionInfo":{"status":"ok","timestamp":1622673255645,"user_tz":420,"elapsed":1626,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}}},"source":["detector = VideoObjectDetection()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"4y9oEcIQpNnu","executionInfo":{"status":"ok","timestamp":1622673258719,"user_tz":420,"elapsed":406,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}}},"source":["detector = ObjectDetection()"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KmCIvOx2fElh"},"source":["## YOLO v3"]},{"cell_type":"markdown","metadata":{"id":"gizGCAsKfHHJ"},"source":["This function sets the model type of the object detection instance we created to the YOLOv3 model, which means we will be performing our object detection tasks using the pre-trained “YOLOv3” model."]},{"cell_type":"markdown","metadata":{"id":"TkupKxXDgD-d"},"source":["We can also set the model either to RetinaNet with **.setModelTypeAsRetinaNet()** or to TinyYOLOv3 with **.setModelTypeAsTinyYOLOv3()**"]},{"cell_type":"code","metadata":{"id":"iUIfQDGzZ0g5","executionInfo":{"status":"ok","timestamp":1622673264275,"user_tz":420,"elapsed":231,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}}},"source":["detector.setModelTypeAsYOLOv3()"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0hHQkwpgfY1H"},"source":["## Mount the drive to import yolo.h5 and the video"]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"kukVv8ubaXWl","executionInfo":{"status":"ok","timestamp":1622673291789,"user_tz":420,"elapsed":18358,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"7fa84538-60c1-40b8-d7b5-5f520eb47b43"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BIZuILNJg36Q"},"source":["## The model file\n","This function accepts a string which must be the path to the model file we downloaded"]},{"cell_type":"code","metadata":{"id":"J2C88FoldrHP","executionInfo":{"status":"ok","timestamp":1622673295216,"user_tz":420,"elapsed":277,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}}},"source":["detector.setModelPath(\"/content/drive/MyDrive/YoloWork/data/yolo.h5\")"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GR9XmIAHhQpL"},"source":["## Load the model\n","This function loads the model from the path we specified in the function call above into our object detection instance\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"id":"tkYRCrYEZ6Xd","executionInfo":{"status":"error","timestamp":1622673312279,"user_tz":420,"elapsed":11392,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"7faa16d0-977c-4660-bc49-fc521312f861"},"source":["detector.loadModel()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-7e03f3565a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageai/Detection/__init__.py\u001b[0m in \u001b[0;36mloadModel\u001b[0;34m(self, detection_speed)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__modelType\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"yolov3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__yolo_anchors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumbers_to_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 hsv_tuples = [(x / len(self.numbers_to_names), 1., 1.)\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \"\"\"\n\u001b[1;32m   1182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'keras_version'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m         \u001b[0moriginal_keras_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keras_version'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0moriginal_keras_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqjZBRWRqMRu","executionInfo":{"status":"ok","timestamp":1622673343260,"user_tz":420,"elapsed":229,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"cc13234f-b1f9-4c0f-c673-71ead1493502"},"source":["%cd /content/drive/MyDrive/YoloWork"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/YoloWork\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CKjh2yarqntN","executionInfo":{"status":"ok","timestamp":1622673346955,"user_tz":420,"elapsed":1752,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}}},"source":["import cv2\n","import csv\n","import os\n","from os import path\n","from google.colab.patches import cv2_imshow\n","\n","DIR = 'images'\n","OutDIR = 'converted'\n","yoDIR = 'yoImages'\n","imgFiles = [f for f in os.listdir(DIR) if f.endswith('.png')]\n","for fName in imgFiles:\n","    yoName = os.path.join(yoDIR,fName)\n","    csvName = path.splitext(fName)[0]\n","    csvName = csvName + \".csv\"\n","    print(csvName)\n","    csvFile = open(os.path.join(OutDIR,csvName), 'w', newline='')\n","    csvWriter = csv.writer(csvFile)\n","    inName = os.path.join(DIR,fName)\n","    img = cv2.imread(inName)\n","    detections = detector.detectObjectsFromImage(input_image=inName, output_image_path=yoName, minimum_percentage_probability=25)\n","    for i in range(len(detections)):\n","        if detections[i]['name'] == 'person':\n","            if detections[i]['percentage_probability'] > 95:\n","                box = detections[i]['box_points']\n","                img = cv2.rectangle(img,(box[0], box[1]),(box[2],box[3]),(255,0,0),cv2.FILLED)\n","                csvWriter.writerow(box)\n","    cv2_imshow(img)\n","    outName = os.path.join(OutDIR,fName)\n","    cv2.imwrite(outName,img)\n","    csvFile.close()"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkwwEt5ZjRGv"},"source":["## Detect Objects From Video\n","This is the function that performs object detecttion on a video file or video live-feed after the model has been loaded into the instance we created."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"K4Dyp75tZ81V","executionInfo":{"status":"error","timestamp":1622673453170,"user_tz":420,"elapsed":239,"user":{"displayName":"Sanghyun Suh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiUdCPmMsqijTthZ24MFHldVLUgCbJtwxfzXlusFw=s64","userId":"03325525587695274031"}},"outputId":"ad223383-c6d9-4ef6-a679-7b041018662d"},"source":["video_path = detector.detectObjectsFromVideo(input_file_path=\"/content/drive/MyDrive/YoloWork/Video/IMG_2256.mp4\",\n","                                output_file_path=\"/content/drive/MyDrive/YoloWork/Video/video_output\",\n","                                frames_per_second=29, log_progress=True)"],"execution_count":14,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-87941aa8ee4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m video_path = detector.detectObjectsFromVideo(input_file_path=\"/content/drive/MyDrive/YoloWork/Video/IMG_2256.mp4\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0moutput_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/YoloWork/Video/video_output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 frames_per_second=29, log_progress=True)\n","\u001b[0;31mAttributeError\u001b[0m: 'ObjectDetection' object has no attribute 'detectObjectsFromVideo'"]}]},{"cell_type":"markdown","metadata":{"id":"7bYWXQdakpph"},"source":["– *parameter* **input_file_path** (required if you did not set camera_input) : This refers to the path to the video file you want to detect.\n","\n","— *parameter* **output_file_path** (required if you did not set save_detected_video = False) : This refers to the path to which the detected video will be saved. By default, this functionsaves video .avi format.\n","\n","– *parameter* **frames_per_second** (optional , but recommended) : This parameters allows you to set your desired frames per second for the detected video that will be saved. The default value is 20 but we recommend you set the value that suits your video or camera live-feed.\n","\n","— *parameter* **log_progress** (optional) : Setting this parameter to True shows the progress of the video or live-feed as it is detected in the CLI. It will report every frame detected as it progresses. The default value is False.\n","\n","— *parameter* **camera_input** (optional) : This parameter can be set in replacement of the input_file_path if you want to detect objects in the live-feed of a camera."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"1S3QGpHJlRJU","outputId":"4a06bee3-03db-4cbe-cbab-e84934f77750"},"source":["!ls '/content/gdrive/My Drive/Colab Notebooks/yolo/data'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["complexe.jpg  simple.jpg  video.mp4  video_output.avi  yolo.h5\n"],"name":"stdout"}]}]}